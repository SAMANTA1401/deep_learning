{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  datasets = tfds.load(name='mnist', as_supervised=True)\n",
    "  mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "  BUFFER_SIZE = 10000\n",
    "\n",
    "  BATCH_SIZE_PER_REPLICA = 64\n",
    "  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync\n",
    "\n",
    "  def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "\n",
    "    return image, label\n",
    "\n",
    "  train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "  eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\n",
    "\n",
    "  return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "  with mirrored_strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_path = '/tmp/keras_save.keras'\n",
    "model.save(keras_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_keras_model = tf.keras.models.load_model(keras_model_path)\n",
    "restored_keras_model.fit(train_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_strategy = tf.distribute.OneDeviceStrategy('/cpu:0')\n",
    "with another_strategy.scope():\n",
    "  restored_keras_model_ds = tf.keras.models.load_model(keras_model_path)\n",
    "  restored_keras_model_ds.fit(train_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()  # get a fresh model\n",
    "saved_model_path = '/tmp/tf_save'\n",
    "tf.saved_model.save(model, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FUNCTION_KEY = 'serving_default'\n",
    "loaded = tf.saved_model.load(saved_model_path)\n",
    "inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = eval_dataset.map(lambda image, label: image)\n",
    "for batch in predict_dataset.take(1):\n",
    "  print(inference_func(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple CPUs with virtual devices\n",
    "N_VIRTUAL_DEVICES = 2\n",
    "physical_devices = tf.config.list_physical_devices(\"CPU\")\n",
    "tf.config.set_logical_device_configuration(\n",
    "    physical_devices[0], [tf.config.LogicalDeviceConfiguration() for _ in range(N_VIRTUAL_DEVICES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "0) LogicalDevice(name='/device:CPU:0', device_type='CPU')\n",
      "1) LogicalDevice(name='/device:CPU:1', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "print(\"Available devices:\")\n",
    "for i, device in enumerate(tf.config.list_logical_devices()):\n",
    "  print(\"%d) %s\" % (i, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 16\n",
    "# Create a tf.data.Dataset object.\n",
    "dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(global_batch_size)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "  features, labels = inputs\n",
    "  return labels - 0.3 * features\n",
    "\n",
    "# Iterate over the dataset using the for..in construct.\n",
    "for inputs in dataset:\n",
    "  print(train_step(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 16\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(global_batch_size)\n",
    "# Distribute input using the `experimental_distribute_dataset`.\n",
    "dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n",
    "# 1 global batch of data fed to the model in 1 step.\n",
    "print(next(iter(dist_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset.range(6).batch(4, drop_remainder=False)\n",
    "\n",
    "# Without distribution:\n",
    "# Batch 1: [0, 1, 2, 3]\n",
    "# Batch 2: [4, 5]\n",
    "# With distribution over 2 replicas. The last batch ([4, 5]) is split between 2 replicas.\n",
    "\n",
    "# Batch 1:\n",
    "\n",
    "# Replica 1:[0, 1]\n",
    "# Replica 2:[2, 3]\n",
    "# Batch 2:\n",
    "\n",
    "# Replica 1: [4]\n",
    "# Replica 2: [5]\n",
    "# tf.data.Dataset.range(4).batch(4)\n",
    "\n",
    "# Without distribution:\n",
    "# Batch 1: [0, 1, 2, 3]\n",
    "# With distribution over 5 replicas:\n",
    "# Batch 1:\n",
    "# Replica 1: [0]\n",
    "# Replica 2: [1]\n",
    "# Replica 3: [2]\n",
    "# Replica 4: [3]\n",
    "# Replica 5: []\n",
    "# tf.data.Dataset.range(8).batch(4)\n",
    "\n",
    "# Without distribution:\n",
    "# Batch 1: [0, 1, 2, 3]\n",
    "# Batch 2: [4, 5, 6, 7]\n",
    "# With distribution over 3 replicas:\n",
    "# Batch 1:\n",
    "# Replica 1: [0, 1]\n",
    "# Replica 2: [2, 3]\n",
    "# Replica 3: []\n",
    "# Batch 2:\n",
    "# Replica 1: [4, 5]\n",
    "# Replica 2: [6, 7]\n",
    "# Replica 3: []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker 0:\n",
    "# Batch 1 = Replica 1: [0, 1]\n",
    "# Batch 2 = Replica 1: [2, 3]\n",
    "# Batch 3 = Replica 1: [4]\n",
    "# Batch 4 = Replica 1: [5]\n",
    "# Worker 1:\n",
    "# Batch 1 = Replica 2: [6, 7]\n",
    "# Batch 2 = Replica 2: [8, 9]\n",
    "# Batch 3 = Replica 2: [10]\n",
    "# Batch 4 = Replica 2: [11]\n",
    "# DATA: This will autoshard the elements across all the workers. Each of the workers will read the entire dataset and only process the shard assigned to it. All other shards will be discarded. This is generally used if the number of input files is less than the number of workers and you want better sharding of data across all workers. The downside is that the entire dataset will be read on each worker. For example, let us distribute 1 files over 2 workers. File 1 contains [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. Let the total number of replicas in sync be 2.\n",
    "\n",
    "# Worker 0:\n",
    "# Batch 1 = Replica 1: [0, 1]\n",
    "# Batch 2 = Replica 1: [4, 5]\n",
    "# Batch 3 = Replica 1: [8, 9]\n",
    "# Worker 1:\n",
    "# Batch 1 = Replica 2: [2, 3]\n",
    "# Batch 2 = Replica 2: [6, 7]\n",
    "# Batch 3 = Replica 2: [10, 11]\n",
    "# OFF: If you turn off autosharding, each worker will process all the data. For example, let us distribute 1 files over 2 workers. File 1 contains [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. Let the total number of replicas in sync be 2. Then each worker will see the following distribution:\n",
    "\n",
    "# Worker 0:\n",
    "# Batch 1 = Replica 1: [0, 1]\n",
    "# Batch 2 = Replica 1: [2, 3]\n",
    "# Batch 3 = Replica 1: [4, 5]\n",
    "# Batch 4 = Replica 1: [6, 7]\n",
    "# Batch 5 = Replica 1: [8, 9]\n",
    "# Batch 6 = Replica 1: [10, 11]\n",
    "\n",
    "# Worker 1:\n",
    "\n",
    "# Batch 1 = Replica 2: [0, 1]\n",
    "\n",
    "# Batch 2 = Replica 2: [2, 3]\n",
    "\n",
    "# Batch 3 = Replica 2: [4, 5]\n",
    "\n",
    "# Batch 4 = Replica 2: [6, 7]\n",
    "\n",
    "# Batch 5 = Replica 2: [8, 9]\n",
    "\n",
    "# Batch 6 = Replica 2: [10, 11]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
