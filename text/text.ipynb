{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subword tokenizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to generate a subword vocabulary from a dataset, and use it to build a text.BertTokenizer from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "# import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0-dev20241117\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.10\n",
    "# Tensorflow 2.13.0\n",
    "# tensorflow-text 2.13.0\n",
    "# tensorflow-models-official 2.13.1\n",
    "# Everything works with these versions, but I did not find a way to make it work with Python 3.11 atm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-text==2.19 (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-text==2.19\n"
     ]
    }
   ],
   "source": [
    "# pip install -U tensorflow-text==2.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in d:\\a27_years_old\\deep_learning\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\a27_years_old\\deep_learning\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\a27_years_old\\deep_learning\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\a27_years_old\\deep_learning\\venv\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: colorama in d:\\a27_years_old\\deep_learning\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "pwd = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\lenovo\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:14<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:15<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:15<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:15<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:17<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:19<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 15.89s/ url]\n",
      "Extraction completed...: 100%|██████████| 112/112 [00:20<00:00,  5.40 file/s]\n",
      "Dl Size...: 100%|██████████| 124/124 [00:20<00:00,  5.98 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 20.76s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to C:\\Users\\lenovo\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "English:    and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
     ]
    }
   ],
   "source": [
    "for pt, en in train_examples.take(1):\n",
    "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
    "  print(\"English:   \", en.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_examples.map(lambda pt, en: en)\n",
    "train_pt = train_examples.map(lambda pt, en: pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lenovo/nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# # Assume 'tensor' is your TensorFlow object\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# tensor = tf.constant([\"This is a sample sentence.\", \"Another sentence.\"])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_en\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Flatten token list\u001b[39;00m\n\u001b[0;32m     15\u001b[0m flat_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "Cell \u001b[1;32mIn[61], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# # Assume 'tensor' is your TensorFlow object\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# tensor = tf.constant([\"This is a sample sentence.\", \"Another sentence.\"])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_en]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Flatten token list\u001b[39;00m\n\u001b[0;32m     15\u001b[0m flat_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lenovo/nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Assume 'tensor' is your TensorFlow object\n",
    "# tensor = tf.constant([\"This is a sample sentence.\", \"Another sentence.\"])\n",
    "\n",
    "# # Extract text data\n",
    "# text_data = train_en.numpy().astype(str)\n",
    "\n",
    "# Tokenize text\n",
    "tokens = [word_tokenize(sentence) for sentence in train_en]\n",
    "\n",
    "# Flatten token list\n",
    "flat_tokens = [token for sublist in tokens for token in sublist]\n",
    "\n",
    "# Convert to lowercase\n",
    "flat_tokens = [token.lower() for token in flat_tokens]\n",
    "\n",
    "# Remove punctuation and stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "flat_tokens = [token for token in flat_tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "# Generate vocabulary\n",
    "vocabulary = set(flat_tokens)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<_MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.map_op._MapDataset'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# # Assume 'tensor' is your TensorFlow object\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# tensor = tf.constant([\"This is a sample sentence.\", \"Another sentence.\"])\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_en\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Flatten token list\u001b[39;00m\n\u001b[0;32m     10\u001b[0m flat_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (<_MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.map_op._MapDataset'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# # Assume 'tensor' is your TensorFlow object\n",
    "# tensor = tf.constant([\"This is a sample sentence.\", \"Another sentence.\"])\n",
    "\n",
    "# Tokenize text\n",
    "tokens = tf.strings.split(train_en).numpy()\n",
    "\n",
    "# Flatten token list\n",
    "flat_tokens = [token for sublist in tokens for token in sublist]\n",
    "\n",
    "# Convert to lowercase\n",
    "flat_tokens = [token.decode('utf-8').lower() for token in flat_tokens]\n",
    "\n",
    "# Remove punctuation and stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "flat_tokens = [token for token in flat_tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "# Generate vocabulary\n",
    "vocabulary = set(flat_tokens)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lenovo/nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # Load text file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# with open('text_file.txt', 'r') as file:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     text = file.read()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_en\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert to lowercase\u001b[39;00m\n\u001b[0;32m     12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lenovo/nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\a27_YEARS_OLD\\\\deep_learning\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Load text file\n",
    "# with open('text_file.txt', 'r') as file:\n",
    "#     text = file.read()\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(train_en)\n",
    "\n",
    "# Convert to lowercase\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Remove punctuation and stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "# Generate vocabulary\n",
    "vocabulary = set(tokens)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load corpus\n",
    "text = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Remove punctuation and stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "# Generate vocabulary\n",
    "vocabulary = set(tokens)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP models often handle different languages with different character sets. Unicode is a standard encoding system that is used to represent characters from almost all languages. Every Unicode character is encoded using a unique integer code point between 0 and 0x10FFFF. A Unicode string is a sequence of zero or more code points.\n",
    "\n",
    "This tutorial shows how to represent Unicode strings in TensorFlow and manipulate them using Unicode equivalents of standard string ops. It separates Unicode strings into tokens based on script detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic TensorFlow tf.string dtype allows you to build tensors of byte strings. Unicode strings are utf-8 encoded by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'Thanks \\xf0\\x9f\\x98\\x8a'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(u\"Thanks 😊\") #byte code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([u\"You're\", u\"welcome!\"]).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two standard ways to represent a Unicode string in TensorFlow:\n",
    "\n",
    "string scalar — where the sequence of code points is encoded using a known character encoding.\n",
    "int32 vector — where each position contains a single code point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\xe8\\xaf\\xad\\xe8\\xa8\\x80\\xe5\\xa4\\x84\\xe7\\x90\\x86'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unicode string, represented as a UTF-8 encoded string scalar.\n",
    "text_utf8 = tf.constant(u\"语言处理\")  #language processing\n",
    "text_utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\x8b\\xed\\x8a\\x00Y\\x04t\\x06'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unicode string, represented as a UTF-16-BE encoded string scalar.\n",
    "text_utf16be = tf.constant(u\"语言处理\".encode(\"UTF-16-BE\"))\n",
    "text_utf16be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ord() function in Python returns the Unicode code point for a given character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 35821,  35328,  22788,  29702, 128522], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unicode string, represented as a vector of Unicode code points.\n",
    "text_chars = tf.constant([ord(char) for char in u\"语言处理😊\"])\n",
    "text_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([35821, 35328, 22788, 29702], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(text_utf8,\n",
    "                          input_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\x8b\\xed\\x8a\\x00Y\\x04t\\x06'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_transcode(text_utf8,\n",
    "                             input_encoding='UTF8',\n",
    "                             output_encoding='UTF-16-BE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 195, 108, 108, 111]\n",
      "[87, 104, 97, 116, 32, 105, 115, 32, 116, 104, 101, 32, 119, 101, 97, 116, 104, 101, 114, 32, 116, 111, 109, 111, 114, 114, 111, 119]\n",
      "[71, 246, 246, 100, 110, 105, 103, 104, 116]\n",
      "[128522]\n"
     ]
    }
   ],
   "source": [
    "# A batch of Unicode strings, each represented as a UTF8-encoded string.\n",
    "batch_utf8 = [s.encode('UTF-8') for s in\n",
    "              [u'hÃllo', u'What is the weather tomorrow', u'Göödnight', u'😊']]\n",
    "batch_chars_ragged = tf.strings.unicode_decode(batch_utf8,\n",
    "                                               input_encoding='UTF-8')\n",
    "for sentence_chars in batch_chars_ragged.to_list():\n",
    "  print(sentence_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'h\\xc3\\x83llo',\n",
       " b'What is the weather tomorrow',\n",
       " b'G\\xc3\\xb6\\xc3\\xb6dnight',\n",
       " b'\\xf0\\x9f\\x98\\x8a']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   104    195    108    108    111     -1     -1     -1     -1     -1\n",
      "      -1     -1     -1     -1     -1     -1     -1     -1     -1     -1\n",
      "      -1     -1     -1     -1     -1     -1     -1     -1]\n",
      " [    87    104     97    116     32    105    115     32    116    104\n",
      "     101     32    119    101     97    116    104    101    114     32\n",
      "     116    111    109    111    114    114    111    119]\n",
      " [    71    246    246    100    110    105    103    104    116     -1\n",
      "      -1     -1     -1     -1     -1     -1     -1     -1     -1     -1\n",
      "      -1     -1     -1     -1     -1     -1     -1     -1]\n",
      " [128522     -1     -1     -1     -1     -1     -1     -1     -1     -1\n",
      "      -1     -1     -1     -1     -1     -1     -1     -1     -1     -1\n",
      "      -1     -1     -1     -1     -1     -1     -1     -1]]\n"
     ]
    }
   ],
   "source": [
    "batch_chars_padded = batch_chars_ragged.to_tensor(default_value=-1)\n",
    "print(batch_chars_padded.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(indices=tf.Tensor(\n",
       "[[ 0  0]\n",
       " [ 0  1]\n",
       " [ 0  2]\n",
       " [ 0  3]\n",
       " [ 0  4]\n",
       " [ 1  0]\n",
       " [ 1  1]\n",
       " [ 1  2]\n",
       " [ 1  3]\n",
       " [ 1  4]\n",
       " [ 1  5]\n",
       " [ 1  6]\n",
       " [ 1  7]\n",
       " [ 1  8]\n",
       " [ 1  9]\n",
       " [ 1 10]\n",
       " [ 1 11]\n",
       " [ 1 12]\n",
       " [ 1 13]\n",
       " [ 1 14]\n",
       " [ 1 15]\n",
       " [ 1 16]\n",
       " [ 1 17]\n",
       " [ 1 18]\n",
       " [ 1 19]\n",
       " [ 1 20]\n",
       " [ 1 21]\n",
       " [ 1 22]\n",
       " [ 1 23]\n",
       " [ 1 24]\n",
       " [ 1 25]\n",
       " [ 1 26]\n",
       " [ 1 27]\n",
       " [ 2  0]\n",
       " [ 2  1]\n",
       " [ 2  2]\n",
       " [ 2  3]\n",
       " [ 2  4]\n",
       " [ 2  5]\n",
       " [ 2  6]\n",
       " [ 2  7]\n",
       " [ 2  8]\n",
       " [ 3  0]], shape=(43, 2), dtype=int64), values=tf.Tensor(\n",
       "[   104    195    108    108    111     87    104     97    116     32\n",
       "    105    115     32    116    104    101     32    119    101     97\n",
       "    116    104    101    114     32    116    111    109    111    114\n",
       "    114    111    119     71    246    246    100    110    105    103\n",
       "    104    116 128522], shape=(43,), dtype=int32), dense_shape=tf.Tensor([ 4 28], shape=(2,), dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chars_sparse = batch_chars_ragged.to_sparse()\n",
    "batch_chars_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = batch_chars_sparse.dense_shape.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " ['_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " ['_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " ['_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements = [['_' for i in range(ncols)] for j in range(nrows)]\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (row, col), value in zip(batch_chars_sparse.indices.numpy(), batch_chars_sparse.values.numpy()):\n",
    "  elements[row][col] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['104',\n",
       "  '195',\n",
       "  '108',\n",
       "  '108',\n",
       "  '111',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " ['87',\n",
       "  '104',\n",
       "  '97',\n",
       "  '116',\n",
       "  '32',\n",
       "  '105',\n",
       "  '115',\n",
       "  '32',\n",
       "  '116',\n",
       "  '104',\n",
       "  '101',\n",
       "  '32',\n",
       "  '119',\n",
       "  '101',\n",
       "  '97',\n",
       "  '116',\n",
       "  '104',\n",
       "  '101',\n",
       "  '114',\n",
       "  '32',\n",
       "  '116',\n",
       "  '111',\n",
       "  '109',\n",
       "  '111',\n",
       "  '114',\n",
       "  '114',\n",
       "  '111',\n",
       "  '119'],\n",
       " ['71',\n",
       "  '246',\n",
       "  '246',\n",
       "  '100',\n",
       "  '110',\n",
       "  '105',\n",
       "  '103',\n",
       "  '104',\n",
       "  '116',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " ['128522',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_width = max(len(value) for row in elements for value in row)\n",
    "value_lengths = []\n",
    "for row in elements:\n",
    "  for value in row:\n",
    "    value_lengths.append(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_width = max(value_lengths)\n",
    "max_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner join: ', '.join(value.rjust(max_width) for value in row):\n",
    "Iterates over each value in a row.\n",
    "Right-justifies (rjust) each value to max_width characters.\n",
    "Joins values with commas.\n",
    "Outer join: '\\n '.join(...):\n",
    "Iterates over each row.\n",
    "Joins rows with newline characters (\\n).\n",
    "Wrapper: ('[%s]' % ...)\n",
    "Wraps the entire table in square brackets ([])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   104,    195,    108,    108,    111,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _]\n",
      " [    87,    104,     97,    116,     32,    105,    115,     32,    116,    104,    101,     32,    119,    101,     97,    116,    104,    101,    114,     32,    116,    111,    109,    111,    114,    114,    111,    119]\n",
      " [    71,    246,    246,    100,    110,    105,    103,    104,    116,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _]\n",
      " [128522,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _,      _]]\n"
     ]
    }
   ],
   "source": [
    "print('[%s]' % '\\n '.join('[%s]' % ', '.join(value.rjust(max_width) for value in row) for row in elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'cat', b'dog', b'cow'], dtype=object)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_encode([[99, 97, 116], [100, 111, 103], [99, 111, 119]],\n",
    "                          output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "array([b'h\\xc3\\x83llo', b'What is the weather tomorrow',\n",
       "       b'G\\xc3\\xb6\\xc3\\xb6dnight', b'\\xf0\\x9f\\x98\\x8a'], dtype=object)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_encode(batch_chars_ragged, output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "array([b'h\\xc3\\x83llo', b'What is the weather tomorrow',\n",
       "       b'G\\xc3\\xb6\\xc3\\xb6dnight', b'\\xf0\\x9f\\x98\\x8a'], dtype=object)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_encode(\n",
    "    tf.RaggedTensor.from_sparse(batch_chars_sparse),\n",
    "    output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "array([b'h\\xc3\\x83llo', b'What is the weather tomorrow',\n",
       "       b'G\\xc3\\xb6\\xc3\\xb6dnight', b'\\xf0\\x9f\\x98\\x8a'], dtype=object)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_encode(\n",
    "    tf.RaggedTensor.from_tensor(batch_chars_padded, padding=-1),\n",
    "    output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the unit parameter of the tf.strings.length op to indicate how character lengths should be computed. unit defaults to \"BYTE\", but it can be set to other values, such as \"UTF8_CHAR\" or \"UTF16_CHAR\", to determine the number of Unicode codepoints in each encoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 bytes; 8 UTF-8 characters\n"
     ]
    }
   ],
   "source": [
    "# Note that the final character takes up 4 bytes in UTF8.\n",
    "thanks = u'Thanks 😊'.encode('UTF-8')\n",
    "num_bytes = tf.strings.length(thanks).numpy()\n",
    "num_chars = tf.strings.length(thanks, unit='UTF8_CHAR').numpy()\n",
    "print('{} bytes; {} UTF-8 characters'.format(num_bytes, num_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xf0'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, unit='BYTE' (default). Returns a single byte with len=1\n",
    "tf.strings.substr(thanks, pos=7, len=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xf0\\x9f\\x98\\x8a'\n"
     ]
    }
   ],
   "source": [
    "# Specifying unit='UTF8_CHAR', returns a single 4 byte character in this case\n",
    "print(tf.strings.substr(thanks, pos=7, len=1, unit='UTF8_CHAR').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'T', b'h', b'a', b'n', b'k', b's', b' ', b'\\xf0\\x9f\\x98\\x8a'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_split(thanks, 'UTF-8').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At byte offset 0: codepoint 127880\n",
      "At byte offset 4: codepoint 127881\n",
      "At byte offset 8: codepoint 127882\n"
     ]
    }
   ],
   "source": [
    "codepoints, offsets = tf.strings.unicode_decode_with_offsets(u'🎈🎉🎊', 'UTF-8')\n",
    "\n",
    "for (codepoint, offset) in zip(codepoints.numpy(), offsets.numpy()):\n",
    "  print('At byte offset {}: codepoint {}'.format(offset, codepoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align the character tensor generated by tf.strings.unicode_decode with the original string, it's useful to know the offset for where each character begins. The method tf.strings.unicode_decode_with_offsets is similar to unicode_decode, except that it returns a second tensor containing the start offset of each characte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17  8]\n"
     ]
    }
   ],
   "source": [
    "uscript = tf.strings.unicode_script([33464, 1041])  # ['芸', 'Б']\n",
    "\n",
    "print(uscript.numpy())  # [17, 8] == [USCRIPT_HAN, USCRIPT_CYRILLIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[25, 25, 25, 25, 25],\n",
      " [25, 25, 25, 25, 0, 25, 25, 0, 25, 25, 25, 0, 25, 25, 25, 25, 25, 25, 25,\n",
      "  0, 25, 25, 25, 25, 25, 25, 25, 25]                                      ,\n",
      " [25, 25, 25, 25, 25, 25, 25, 25, 25], [0]]>\n"
     ]
    }
   ],
   "source": [
    "print(tf.strings.unicode_script(batch_chars_ragged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation is the task of splitting text into word-like units. This is often easy when space characters are used to separate words, but some languages (like Chinese and Japanese) do not use spaces, and some languages (like German) contain long compounds that must be split in order to analyze their meaning. In web text, different languages and scripts are frequently mixed together, as in \"NY株価\" (New York Stock Exchan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype: string; shape: [num_sentences]\n",
    "#\n",
    "# The sentences to process.  Edit this line to try out different inputs!\n",
    "sentence_texts = [u'Hello, world.', u'世界こんにちは']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 46],\n",
      " [19990, 30028, 12371, 12435, 12395, 12385, 12399]]>\n",
      "<tf.RaggedTensor [[25, 25, 25, 25, 25, 0, 0, 25, 25, 25, 25, 25, 0],\n",
      " [17, 17, 20, 20, 20, 20, 20]]>\n"
     ]
    }
   ],
   "source": [
    "# dtype: int32; shape: [num_sentences, (num_chars_per_sentence)]\n",
    "#\n",
    "# sentence_char_codepoint[i, j] is the codepoint for the j'th character in\n",
    "# the i'th sentence.\n",
    "sentence_char_codepoint = tf.strings.unicode_decode(sentence_texts, 'UTF-8')\n",
    "print(sentence_char_codepoint)\n",
    "\n",
    "# dtype: int32; shape: [num_sentences, (num_chars_per_sentence)]\n",
    "#\n",
    "# sentence_char_scripts[i, j] is the Unicode script of the j'th character in\n",
    "# the i'th sentence.\n",
    "sentence_char_script = tf.strings.unicode_script(sentence_char_codepoint)\n",
    "print(sentence_char_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p 1: First Character Masking\n",
    "tf.fill([sentence_char_script.nrows(), 1], True):\n",
    "Creates a tensor with shape (num_sentences, 1) filled with True.\n",
    "Assumes the first character of each sentence starts a word.\n",
    "Step 2: Subsequent Character Masking\n",
    "tf.not_equal(sentence_char_script[:, 1:], sentence_char_script[:, :-1]):\n",
    "Compares adjacent characters in each sentence.\n",
    "Returns True where characters differ (indicating word boundaries).\n",
    "Shape: (num_sentences, num_chars - 1)\n",
    "Step 3: Concatenation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.concat(..., axis=1):\n",
    "Combines the first character mask and subsequent character mask.\n",
    "Resulting shape: (num_sentences, num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  5  7 12 13 15], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# dtype: bool; shape: [num_sentences, (num_chars_per_sentence)]\n",
    "#\n",
    "# sentence_char_starts_word[i, j] is True if the j'th character in the i'th\n",
    "# sentence is the start of a word.\n",
    "sentence_char_starts_word = tf.concat(\n",
    "    [tf.fill([sentence_char_script.nrows(), 1], True),\n",
    "     tf.not_equal(sentence_char_script[:, 1:], sentence_char_script[:, :-1])],\n",
    "    axis=1)\n",
    "\n",
    "# dtype: int64; shape: [num_words]\n",
    "#\n",
    "# word_starts[i] is the index of the character that starts the i'th word (in\n",
    "# the flattened list of characters from all sentences).\n",
    "word_starts = tf.squeeze(tf.where(sentence_char_starts_word.values), axis=1)\n",
    "print(word_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[True, False, False, False, False, True, False, True, False, False, False,\n",
       "  False, True]                                                             ,\n",
       " [True, False, True, False, False, False, False]]>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_char_starts_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[72, 101, 108, 108, 111], [44, 32], [119, 111, 114, 108, 100], [46],\n",
      " [19990, 30028], [12371, 12435, 12395, 12385, 12399]]>\n"
     ]
    }
   ],
   "source": [
    "# dtype: int32; shape: [num_words, (num_chars_per_word)]\n",
    "#\n",
    "# word_char_codepoint[i, j] is the codepoint for the j'th character in the\n",
    "# i'th word.\n",
    "word_char_codepoint = tf.RaggedTensor.from_row_starts(\n",
    "    values=sentence_char_codepoint.values,\n",
    "    row_starts=word_starts)\n",
    "print(word_char_codepoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[72, 101, 108, 108, 111], [44, 32], [119, 111, 114, 108, 100], [46]],\n",
      " [[19990, 30028], [12371, 12435, 12395, 12385, 12399]]]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[b'Hello', b', ', b'world', b'.'],\n",
       " [b'\\xe4\\xb8\\x96\\xe7\\x95\\x8c',\n",
       "  b'\\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtype: int64; shape: [num_sentences]\n",
    "#\n",
    "# sentence_num_words[i] is the number of words in the i'th sentence.\n",
    "sentence_num_words = tf.reduce_sum(\n",
    "    tf.cast(sentence_char_starts_word, tf.int64),\n",
    "    axis=1)\n",
    "\n",
    "# dtype: int32; shape: [num_sentences, (num_words_per_sentence), (num_chars_per_word)]\n",
    "#\n",
    "# sentence_word_char_codepoint[i, j, k] is the codepoint for the k'th character\n",
    "# in the j'th word in the i'th sentence.\n",
    "sentence_word_char_codepoint = tf.RaggedTensor.from_row_lengths(\n",
    "    values=word_char_codepoint,\n",
    "    row_lengths=sentence_num_words)\n",
    "print(sentence_word_char_codepoint)\n",
    "\n",
    "tf.strings.unicode_encode(sentence_word_char_codepoint, 'UTF-8').to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
