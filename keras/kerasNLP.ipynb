{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KerasNLP is a natural language processing library that supports users through their entire development cycle. Our workflows are built from modular components that have state-of-the-art preset weights and architectures when used out-of-the-box and are easily customizable when more control is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q --upgrade keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "# Use mixed precision to speed up all training in this guide.\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "# !# Remove unsupervised examples\n",
    "# !rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'D:/a27_YEARS_OLD/deep_learning/tensoflow/aclImdb_v1/aclImdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "imdb_train = keras.utils.text_dataset_from_directory(\n",
    "    data_path + 'train',\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "imdb_test = keras.utils.text_dataset_from_directory(\n",
    "    data_path + 'test',\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was the best documentary I've ever seen!! I just saw Lords of Dogtown and wanted to know more about Stacy Peralta, and was surprised and happy to find out this was one of his films as well. Great Job Stacy! I was kicking back at work last week, bored O*&^%less and this movie came on. Growing up in Orange County in the 80's I surfed up and down the local beaches and so did my dad when he was a teenager. I grew up at the beach, my parents took me every weekend, I body surfed, boogeyboarded then moved up from there. This movie just captivated me. It was way before my time but it was awesome to see what these guys went through..TRUE PIONEERS! This movie is a collectors item.\">, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# Inspect first review\n",
    "# Format is (review text tensor, label tensor)\n",
    "print(imdb_train.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment classifier positive negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'encoders', 'layers']\n"
     ]
    }
   ],
   "source": [
    "print(dir(keras_nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_nlp.layers' has no attribute 'BertClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_nlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBertClassifier\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_preset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_tiny_en_uncased_sst2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Note: batched inputs expected so must wrap string in iterable\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras_nlp.layers' has no attribute 'BertClassifier'"
     ]
    }
   ],
   "source": [
    "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
    "# Note: batched inputs expected so must wrap string in iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.predict([\"I love modular workflows in keras-nlp!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.evaluate(imdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    "    num_classes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(\n",
    "    imdb_train,\n",
    "    validation_data=imdb_test,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    "    sequence_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessor to every sample of train and test data using `map()`.\n",
    "# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n",
    "# https://www.tensorflow.org/guide/data_performance for details.\n",
    "\n",
    "# Note: only call `cache()` if you training data fits in CPU memory!\n",
    "imdb_train_cached = (\n",
    "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "imdb_test_cached = (\n",
    "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
    "    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n",
    ")\n",
    "classifier.fit(\n",
    "    imdb_train_cached,\n",
    "    validation_data=imdb_test_cached,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\n",
    "tokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiSegmentPacker is a layer in KerasNLP (part of TensorFlow) that packs multiple segments of text into a single input sequence, preparing data for BERT-style transformer models.\n",
    "What does MultiSegmentPacker do?\n",
    "It takes multiple segments of text (e.g., question and answer pairs) and:\n",
    "Concatenates them into a single sequence.\n",
    "Adds special tokens: start_value (CLS) and end_value (SEP) to indicate segment boundaries.\n",
    "Pads the sequence to a uniform length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own packer or use one of our `Layers`\n",
    "packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "    start_value=tokenizer.cls_token_id,\n",
    "    end_value=tokenizer.sep_token_id,\n",
    "    # Note: This cannot be longer than the preset's `sequence_length`, and there\n",
    "    # is no check for a custom preprocessor!\n",
    "    sequence_length=64,\n",
    ")\n",
    "\n",
    "# packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "#     start_value=tokenizer.cls_token_id,  # 101\n",
    "#     end_value=tokenizer.sep_token_id     # 102\n",
    "# )\n",
    "\n",
    "# # Input segments\n",
    "# segment1 = tf.constant([1, 2, 3])  # \"Hello\"\n",
    "# segment2 = tf.constant([4, 5, 6])  # \"World\"\n",
    "\n",
    "# # Pack segments\n",
    "# packed_sequence = packer([segment1, segment2])\n",
    "\n",
    "# print(packed_sequence)  # [101, 1, 2, 3, 102, 4, 5, 6, 102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function that takes a text sample `x` and its\n",
    "# corresponding label `y` as input and converts the\n",
    "# text into a format suitable for input into a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(x, y):\n",
    "    token_ids, segment_ids = packer(tokenizer(x))\n",
    "    x = {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_preprocessed = imdb_train.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "imdb_test_preprocessed = imdb_test.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Preprocessed example\n",
    "print(imdb_train_preprocessed.unbatch().take(1).get_single_element())\n",
    "\n",
    "# imdb_train_preprocessed.unbatch(): Removes batch dimensions from the dataset.\n",
    "# .take(1): Returns a dataset containing only the first element.\n",
    "# .get_single_element(): Returns the first element of the dataset as a numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
    "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_preprocessed = (\n",
    "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "imdb_test_preprocessed = (\n",
    "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# This line of code is optimizing the imdb_train dataset by applying three key transformations:\n",
    "# Mapping: Applying a preprocessing function (preprocessor) to each element.\n",
    "# Caching: Storing the preprocessed data in memory for faster access.\n",
    "# Prefetching: Overlapping computation and I/O to improve performance.\n",
    "\n",
    "# imdb_train.map(preprocessor, tf.data.AUTOTUNE): Applies the preprocessor function to each element in imdb_train.\n",
    "# tf.data.AUTOTUNE: Automatically adjusts parallelism (number of threads) for optimal performance.\n",
    "\n",
    "# .cache(): Stores the preprocessed data in memory, so subsequent iterations can access it faster.\n",
    "# .prefetch(tf.data.AUTOTUNE): Overlaps computation and I/O, loading the next batch while the current one is processed.\n",
    "# tf.data.AUTOTUNE: Automatically adjusts prefetching buffer size for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = False\n",
    "inputs = backbone.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = backbone(inputs)[\"sequence_output\"]\n",
    "# inputs: Input data, often a tensor or dictionary containing input IDs, attention masks, and token type IDs.\n",
    "# sequence_output: A tensor representing the last hidden state of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    sequence = keras_nlp.layers.TransformerEncoder(\n",
    "        num_heads=2,\n",
    "        intermediate_dim=512,\n",
    "        dropout=0.1,\n",
    "    )(sequence)\n",
    "\n",
    "#This code applies a Transformer Encoder layer twice to the input sequence, enhancing its representation.\n",
    "# TransformerEncoder parameters\n",
    "# num_heads=2: Number of attention heads.\n",
    "# intermediate_dim=512: Dimensionality of the intermediate (feed-forward) layer.\n",
    "# dropout=0.1: Dropout rate (randomly sets 10% of weights to zero).\n",
    "# TransformerEncoder architecture\n",
    "# Self-Attention: Computes attention weights between sequence elements.\n",
    "# Feed-Forward Network (FFN): Transforms attention-weighted outputs.\n",
    "# Layer Normalization: Normalizes activations.\n",
    "# Residual Connection: Adds input to output.\n",
    "# Applying TransformerEncoder twice\n",
    "# First pass: Processes input sequence, capturing local dependencies.\n",
    "# Second pass: Refines output from the first pass, capturing more complex dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use [CLS] token output to classify\n",
    "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n",
    "# keras.layers.Dense(2):\n",
    "# Creates a Dense layer with 2 output units\n",
    "# sequence[:, backbone.cls_token_index, :]:\n",
    "# sequence: The sequence output from the backbone model.\n",
    "# backbone.cls_token_index: The index of the CLS token in the sequence (usually 0).\n",
    "# :: Selects all features (hidden dimensions) for the CLS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.AdamW(5e-5),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    jit_compile=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(\n",
    "    imdb_train_preprocessed,\n",
    "    validation_data=imdb_test_preprocessed,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All BERT `en` models have the same vocabulary, so reuse preprocessor from\n",
    "# \"bert_tiny_en_uncased\"\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    "    sequence_length=256,\n",
    ")\n",
    "packer = preprocessor.packer\n",
    "tokenizer = preprocessor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.Layer to replace some input tokens with the \"[MASK]\" token\n",
    "masker = keras_nlp.layers.MaskedLMMaskGenerator(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    mask_selection_rate=0.25,\n",
    "    mask_selection_length=64,\n",
    "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
    "    unselectable_token_ids=[\n",
    "        tokenizer.token_to_id(x) for x in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]\n",
    "    ],\n",
    ")\n",
    "# Randomly selects tokens to mask based on mask_selection_rate.\n",
    "# Replaces selected tokens with [MASK] token.\n",
    "# Ensures unselectable_token_ids are not masked.\n",
    "# [CLS] This is a sample sentence [SEP].\n",
    "# [CLS] This [MASK] a sample [MASK] [SEP].\n",
    "\n",
    "# Improved language understanding: Masked LM training enhances language model's ability to predict missing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(inputs, label):\n",
    "    inputs = preprocessor(inputs)\n",
    "    masked_inputs = masker(inputs[\"token_ids\"])\n",
    "    # Split the masking layer outputs into a (features, labels, and weights)\n",
    "    # tuple that we can use with keras.Model.fit().\n",
    "    features = {\n",
    "        \"token_ids\": masked_inputs[\"token_ids\"],\n",
    "        \"segment_ids\": inputs[\"segment_ids\"],\n",
    "        \"padding_mask\": inputs[\"padding_mask\"],\n",
    "        \"mask_positions\": masked_inputs[\"mask_positions\"],\n",
    "    }\n",
    "    labels = masked_inputs[\"mask_ids\"]\n",
    "    weights = masked_inputs[\"mask_weights\"]\n",
    "    return features, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_ds = imdb_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "pretrain_val_ds = imdb_test.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Tokens with ID 103 are \"masked\"\n",
    "print(pretrain_ds.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT backbone\n",
    "backbone = keras_nlp.models.BertBackbone(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    num_layers=2,\n",
    "    num_heads=2,\n",
    "    hidden_dim=128,\n",
    "    intermediate_dim=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language modeling head\n",
    "mlm_head = keras_nlp.layers.MaskedLMHead(\n",
    "    token_embedding=backbone.token_embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"token_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"token_ids\"),\n",
    "    \"segment_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"segment_ids\"),\n",
    "    \"padding_mask\": keras.Input(shape=(None,), dtype=tf.int32, name=\"padding_mask\"),\n",
    "    \"mask_positions\": keras.Input(shape=(None,), dtype=tf.int32, name=\"mask_positions\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoded token sequence\n",
    "sequence = backbone(inputs)[\"sequence_output\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict an output word for each masked input token.\n",
    "# We use the input token embedding to project from our encoded vectors to\n",
    "# vocabulary logits, which has been shown to improve training efficiency.\n",
    "outputs = mlm_head(sequence, mask_positions=inputs[\"mask_positions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile our pretraining model.\n",
    "pretraining_model = keras.Model(inputs, outputs)\n",
    "pretraining_model.summary()\n",
    "pretraining_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=5e-4),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    jit_compile=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain on IMDB dataset\n",
    "pretraining_model.fit(\n",
    "    pretrain_ds,\n",
    "    validation_data=pretrain_val_ds,\n",
    "    epochs=3,  # Increase to 6 for higher accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train your own transformer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    imdb_train.map(lambda x, y: x),\n",
    "    vocabulary_size=20_000,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    "    oov_token=\"[UNK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packer = keras_nlp.layers.StartEndPacker(\n",
    "    start_value=tokenizer.token_to_id(\"[START]\"),\n",
    "    end_value=tokenizer.token_to_id(\"[END]\"),\n",
    "    pad_value=tokenizer.token_to_id(\"[PAD]\"),\n",
    "    sequence_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(x, y):\n",
    "    token_ids = packer(tokenizer(x))\n",
    "    return token_ids, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imdb_preproc_train_ds = imdb_train.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "imdb_preproc_val_ds = imdb_test.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imdb_preproc_train_ds.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id_input = keras.Input(\n",
    "    shape=(None,),\n",
    "    dtype=\"int32\",\n",
    "    name=\"token_ids\",\n",
    ")\n",
    "\n",
    "outputs = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=len(vocab),\n",
    "    sequence_length=packer.sequence_length,\n",
    "    embedding_dim=64,\n",
    ")(token_id_input)\n",
    "\n",
    "outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=2,\n",
    "    intermediate_dim=128,\n",
    "    dropout=0.1,\n",
    ")(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"[START]\" token to classify\n",
    "outputs = keras.layers.Dense(2)(outputs[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(\n",
    "    inputs=token_id_input,\n",
    "    outputs=outputs,\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.AdamW(5e-5),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    jit_compile=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    imdb_preproc_train_ds,\n",
    "    validation_data=imdb_preproc_val_ds,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences, labels):\n",
    "    return tokenizer(sentences), labels\n",
    "\n",
    "\n",
    "# We use prefetch() to pre-compute preprocessed batches on the fly on our CPU.\n",
    "finetune_ds = sst_train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "finetune_val_ds = sst_val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview a single input example.\n",
    "print(finetune_val_ds.take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing params.\n",
    "PRETRAINING_BATCH_SIZE = 128\n",
    "FINETUNING_BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 128\n",
    "MASK_RATE = 0.25\n",
    "PREDICTIONS_PER_SEQ = 32\n",
    "\n",
    "# Model params.\n",
    "NUM_LAYERS = 3\n",
    "MODEL_DIM = 256\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "NORM_EPSILON = 1e-5\n",
    "\n",
    "# Training params.\n",
    "PRETRAINING_LEARNING_RATE = 5e-4\n",
    "PRETRAINING_EPOCHS = 8\n",
    "FINETUNING_LEARNING_RATE = 5e-5\n",
    "FINETUNING_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the encoder model from disk so we can restart fine-tuning from scratch.\n",
    "encoder_model = keras.models.load_model(\"encoder_model.keras\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take as input the tokenized input.\n",
    "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and pool the tokens.\n",
    "encoded_tokens = encoder_model(inputs)\n",
    "pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict an output label.\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(pooled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile our fine-tuning model.\n",
    "finetuning_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(FINETUNING_LEARNING_RATE),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the model for the SST-2 task.\n",
    "finetuning_model.fit(\n",
    "    finetune_ds,\n",
    "    validation_data=finetune_val_ds,\n",
    "    epochs=FINETUNING_EPOCHS,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
