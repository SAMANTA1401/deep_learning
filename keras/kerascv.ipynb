{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection is the process of identifying, classifying, and localizing objects within a given image. Typically, your inputs are images, and your labels are bounding boxes with optional class labels. Object detection can be thought of as an extension of classification, however instead of one class label for the image, you must detect and localize an arbitrary number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "from tensorflow import data as tf_data\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from keras_cv import bounding_box\n",
    "import os\n",
    "from keras_cv import visualization\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = [height, width, 3]\n",
    "bounding_boxes = {\n",
    "  \"classes\": [0], # 0 is an arbitrary class ID representing \"cat\"\n",
    "  \"boxes\": [[0.25, 0.4, .15, .1]]\n",
    "   # bounding box is in \"rel_xywh\" format\n",
    "   # so 0.25 represents the start of the bounding box 25% of\n",
    "   # the way across the image.\n",
    "   # The .15 represents that the width is 15% of the image width.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras_cv.models.YOLOV8Detector.from_preset(\n",
    "    \"yolo_v8_m_pascalvoc\", bounding_box_format=\"xywh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = {\n",
    "  \"classes\": [num_boxes],\n",
    "  \"boxes\": [num_boxes, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_file(origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://i.imgur.com/gCNcJJI.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mload_img(filepath)\n\u001b[0;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "filepath = keras.utils.get_file(origin=\"https://i.imgur.com/gCNcJJI.jpg\")\n",
    "image = keras.utils.load_img(filepath)\n",
    "image = np.array(image)\n",
    "\n",
    "\n",
    "visualization.plot_image_gallery(\n",
    "    np.array([image]), # in matrix format\n",
    "    value_range=(0, 255),\n",
    "    rows=1,\n",
    "    cols=1,\n",
    "    scale=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the YOLOV8Detector architecture with a ResNet50 backbone, you'll need to resize your image to a size that is divisible by 64. This is to ensure compatibility with the number of downscaling operations done by the convolution layers in the ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_resizing = keras_cv.layers.Resizing(\n",
    "    640, 640, pad_to_aspect_ratio=True, bounding_box_format=\"xywh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = inference_resizing([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = [\n",
    "    \"Aeroplane\",\n",
    "    \"Bicycle\",\n",
    "    \"Bird\",\n",
    "    \"Boat\",\n",
    "    \"Bottle\",\n",
    "    \"Bus\",\n",
    "    \"Car\",\n",
    "    \"Cat\",\n",
    "    \"Chair\",\n",
    "    \"Cow\",\n",
    "    \"Dining Table\",\n",
    "    \"Dog\",\n",
    "    \"Horse\",\n",
    "    \"Motorbike\",\n",
    "    \"Person\",\n",
    "    \"Potted Plant\",\n",
    "    \"Sheep\",\n",
    "    \"Sofa\",\n",
    "    \"Train\",\n",
    "    \"Tvmonitor\",\n",
    "    \"Total\",\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Aeroplane',\n",
       " 1: 'Bicycle',\n",
       " 2: 'Bird',\n",
       " 3: 'Boat',\n",
       " 4: 'Bottle',\n",
       " 5: 'Bus',\n",
       " 6: 'Car',\n",
       " 7: 'Cat',\n",
       " 8: 'Chair',\n",
       " 9: 'Cow',\n",
       " 10: 'Dining Table',\n",
       " 11: 'Dog',\n",
       " 12: 'Horse',\n",
       " 13: 'Motorbike',\n",
       " 14: 'Person',\n",
       " 15: 'Potted Plant',\n",
       " 16: 'Sheep',\n",
       " 17: 'Sofa',\n",
       " 18: 'Train',\n",
       " 19: 'Tvmonitor',\n",
       " 20: 'Total'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pretrained_model.predict(image_batch)\n",
    "# y_pred is a bounding box Tensor:\n",
    "# {\"classes\": ..., boxes\": ...}\n",
    "visualization.plot_bounding_box_gallery(\n",
    "    image_batch,\n",
    "    value_range=(0, 255),\n",
    "    rows=1,\n",
    "    cols=1,\n",
    "    y_pred=y_pred,\n",
    "    scale=5,\n",
    "    font_scale=0.7,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    class_mapping=class_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to support this easy and intuitive inference workflow, KerasCV performs non-max suppression inside of the YOLOV8Detector class. Non-max suppression is a traditional computing algorithm that solves the problem of a model detecting multiple boxes for the same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following NonMaxSuppression layer is equivalent to disabling the operation\n",
    "prediction_decoder = keras_cv.layers.NonMaxSuppression(\n",
    "    bounding_box_format=\"xywh\",\n",
    "    from_logits=True,\n",
    "    iou_threshold=1.0,\n",
    "    confidence_threshold=0.0,\n",
    ")\n",
    "pretrained_model = keras_cv.models.YOLOV8Detector.from_preset(\n",
    "    \"yolo_v8_m_pascalvoc\",\n",
    "    bounding_box_format=\"xywh\",\n",
    "    prediction_decoder=prediction_decoder,\n",
    ")\n",
    "\n",
    "y_pred = pretrained_model.predict(image_batch)\n",
    "visualization.plot_bounding_box_gallery(\n",
    "    image_batch,\n",
    "    value_range=(0, 255),\n",
    "    rows=1,\n",
    "    cols=1,\n",
    "    y_pred=y_pred,\n",
    "    scale=5,\n",
    "    font_scale=0.7,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    class_mapping=class_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a custom object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "bounding_boxes = {\n",
    "    # num_boxes may be a Ragged dimension\n",
    "    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n",
    "    'classes': Tensor(shape=[batch, num_boxes])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = {\n",
    "    # num_boxes may be a Ragged dimension\n",
    "    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n",
    "    'classes': Tensor(shape=[batch, num_boxes])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "\n",
    "def unpackage_raw_tfds_inputs(inputs, bounding_box_format):\n",
    "    image = inputs[\"image\"]\n",
    "    boxes = keras_cv.bounding_box.convert_format(\n",
    "        inputs[\"objects\"][\"bbox\"],\n",
    "        images=image,\n",
    "        source=\"rel_yxyx\",\n",
    "        target=bounding_box_format,\n",
    "    )\n",
    "    bounding_boxes = {\n",
    "        \"classes\": inputs[\"objects\"][\"label\"],\n",
    "        \"boxes\": boxes,\n",
    "    }\n",
    "    return {\"images\": image, \"bounding_boxes\": bounding_boxes}\n",
    "\n",
    "\n",
    "def load_pascal_voc(split, dataset, bounding_box_format):\n",
    "    ds = tfds.load(dataset, split=split, with_info=False, shuffle_files=True)\n",
    "    ds = ds.map(\n",
    "        lambda x: unpackage_raw_tfds_inputs(x, bounding_box_format=bounding_box_format),\n",
    "        num_parallel_calls=tf_data.AUTOTUNE,\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = load_pascal_voc(\n",
    "    split=\"train\", dataset=\"voc/2007\", bounding_box_format=\"xywh\"\n",
    ")\n",
    "eval_ds = load_pascal_voc(split=\"test\", dataset=\"voc/2007\", bounding_box_format=\"xywh\")\n",
    "\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, ds_info = your_data_loader.load(\n",
    "    split='train', bounding_box_format='xywh', batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(\n",
    "    train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(\n",
    "    eval_ds,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    value_range=(0, 255),\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    # If you are not running your experiment on a local machine, you can also\n",
    "    # make `visualize_dataset()` dump the plot to a file using `path`:\n",
    "    # path=\"eval.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most challenging tasks when constructing object detection pipelines is data augmentation. Image augmentation techniques must be aware of the underlying bounding boxes, and must update them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenters = [\n",
    "    keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n",
    "    keras_cv.layers.JitteredResize(\n",
    "        target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def create_augmenter_fn(augmenters):\n",
    "    def augmenter_fn(inputs):\n",
    "        for augmenter in augmenters:\n",
    "            inputs = augmenter(inputs)\n",
    "        return inputs\n",
    "\n",
    "    return augmenter_fn\n",
    "\n",
    "\n",
    "augmenter_fn = create_augmenter_fn(augmenters)\n",
    "\n",
    "train_ds = train_ds.map(augmenter_fn, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "visualize_dataset(\n",
    "    train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_resizing = keras_cv.layers.Resizing(\n",
    "    640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True\n",
    ")\n",
    "eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(\n",
    "    eval_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], bounding_box.to_dense(\n",
    "        inputs[\"bounding_boxes\"], max_boxes=32\n",
    "    )\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.prefetch(tf_data.AUTOTUNE)\n",
    "eval_ds = eval_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.005\n",
    "# including a global_clipnorm is extremely important in object detection tasks\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.compile(\n",
    "    classification_loss=\"binary_crossentropy\",\n",
    "    box_loss=\"ciou\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_metrics_callback = keras_cv.callbacks.PyCOCOCallback(\n",
    "    eval_ds.take(20), bounding_box_format=\"xywh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras_cv.models.YOLOV8Detector.from_preset(\n",
    "    \"resnet50_imagenet\",\n",
    "    # For more info on supported bounding box formats, visit\n",
    "    # https://keras.io/api/keras_cv/bounding_box/\n",
    "    bounding_box_format=\"xywh\",\n",
    "    num_classes=20,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    classification_loss=\"binary_crossentropy\",\n",
    "    box_loss=\"ciou\",\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds.take(20),\n",
    "    # Run for 10-35~ epochs to achieve good scores.\n",
    "    epochs=1,\n",
    "    callbacks=[coco_metrics_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_ds = eval_ds.unbatch()\n",
    "visualization_ds = visualization_ds.ragged_batch(16)\n",
    "visualization_ds = visualization_ds.shuffle(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(model, dataset, bounding_box_format):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    y_pred = model.predict(images)\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=4,\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "        class_mapping=class_mapping,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prediction_decoder = keras_cv.layers.NonMaxSuppression(\n",
    "    bounding_box_format=\"xywh\",\n",
    "    from_logits=True,\n",
    "    iou_threshold=0.5,\n",
    "    confidence_threshold=0.75,\n",
    ")\n",
    "\n",
    "visualize_detections(model, dataset=visualization_ds, bounding_box_format=\"xywh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion = keras_cv.models.StableDiffusionV2(512, 512)\n",
    "\n",
    "\n",
    "images = stable_diffusion.text_to_image(\n",
    "    prompt=\"A zoomed out photograph of a cool looking cat.  The cat stands in a beautiful forest\",\n",
    "    negative_prompt=\"unrealistic, bad looking, malformed\",\n",
    "    batch_size=4,\n",
    "    seed=1231,\n",
    ")\n",
    "\n",
    "encoded_predictions = model(images)\n",
    "\n",
    "\n",
    "y_pred = model.decode_predictions(encoded_predictions, images)\n",
    "\n",
    "\n",
    "visualization.plot_bounding_box_gallery(\n",
    "    images,\n",
    "    value_range=(0, 255),\n",
    "    y_pred=y_pred,\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    scale=5,\n",
    "    font_scale=0.7,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    class_mapping=class_mapping,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
