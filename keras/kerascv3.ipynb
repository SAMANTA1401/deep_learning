{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will show how to generate novel images based on a text prompt using the KerasCV implementation of stability.ai's text-to-image model, Stable Diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate novel images based on a text prompt using the KerasCV implementation of stability.ai's text-to-image model, Stable Diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import keras_cv\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
     ]
    }
   ],
   "source": [
    "model = keras_cv.models.StableDiffusion(\n",
    "    img_width=512, img_height=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1381: UserWarning: Layer 'clip_encoder_layer_1' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling CLIPAttention.call().\n",
      "\n",
      "\u001b[1mpred must not be a Python bool\u001b[0m\n",
      "\n",
      "Arguments received by CLIPAttention.call():\n",
      "  • inputs=tf.Tensor(shape=(None, 77, 768), dtype=float32)\n",
      "  • attention_mask=None''\n",
      "  warnings.warn(\n",
      "d:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'clip_encoder_layer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling CLIPEncoderLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'clip_encoder_layer_1' (of type CLIPEncoderLayer). Either the `CLIPEncoderLayer.call()` method is incorrect, or you need to implement the `CLIPEncoderLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling CLIPAttention.call().\n\n\u001b[1mpred must not be a Python bool\u001b[0m\n\nArguments received by CLIPAttention.call():\n  • inputs=tf.Tensor(shape=(None, 77, 768), dtype=float32)\n  • attention_mask=None\u001b[0m\n\nArguments received by CLIPEncoderLayer.call():\n  • args=('<KerasTensor shape=(None, 77, 768), dtype=float32, sparse=False, name=keras_tensor_3>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphotograph of an astronaut riding a horse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras_cv\\src\\models\\stable_diffusion\\stable_diffusion.py:83\u001b[0m, in \u001b[0;36mStableDiffusionBase.text_to_image\u001b[1;34m(self, prompt, negative_prompt, batch_size, num_steps, unconditional_guidance_scale, seed)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_to_image\u001b[39m(\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     76\u001b[0m     prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     82\u001b[0m ):\n\u001b[1;32m---> 83\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_image(\n\u001b[0;32m     86\u001b[0m         encoded_text,\n\u001b[0;32m     87\u001b[0m         negative_prompt\u001b[38;5;241m=\u001b[39mnegative_prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m     92\u001b[0m     )\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras_cv\\src\\models\\stable_diffusion\\stable_diffusion.py:125\u001b[0m, in \u001b[0;36mStableDiffusionBase.encode_text\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    122\u001b[0m phrase \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m49407\u001b[39m] \u001b[38;5;241m*\u001b[39m (MAX_PROMPT_LENGTH \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    123\u001b[0m phrase \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor([phrase], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_on_batch(\n\u001b[0;32m    126\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: phrase, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pos_ids()}\n\u001b[0;32m    127\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras_cv\\src\\models\\stable_diffusion\\stable_diffusion.py:427\u001b[0m, in \u001b[0;36mStableDiffusion.text_encoder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"text_encoder returns the text encoder with pretrained weights.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03mCan be overriden for tasks like textual inversion where the text encoder\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03mneeds to be modified.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mTextEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMAX_PROMPT_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit_compile:\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_encoder\u001b[38;5;241m.\u001b[39mcompile(jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras_cv\\src\\models\\stable_diffusion\\text_encoder.py:33\u001b[0m, in \u001b[0;36mTextEncoder.__init__\u001b[1;34m(self, max_length, vocab_size, name, download_weights)\u001b[0m\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m CLIPEmbedding(vocab_size, \u001b[38;5;241m768\u001b[39m, max_length)([tokens, positions])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPEncoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquick_gelu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m embedded \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayerNormalization(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)(x)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m([tokens, positions], embedded, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras_cv\\src\\models\\stable_diffusion\\text_encoder.py:102\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    100\u001b[0m residual \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(inputs)\n\u001b[1;32m--> 102\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m    104\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32md:\\a27_YEARS_OLD\\deep_learning\\venv\\Lib\\site-packages\\keras_cv\\src\\models\\stable_diffusion\\text_encoder.py:136\u001b[0m, in \u001b[0;36mCLIPAttention.call\u001b[1;34m(self, inputs, attention_mask)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausal:\n\u001b[0;32m    135\u001b[0m     length \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mshape(inputs)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 136\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m _, tgt_len, embed_dim \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    143\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(inputs) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling CLIPEncoderLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'clip_encoder_layer_1' (of type CLIPEncoderLayer). Either the `CLIPEncoderLayer.call()` method is incorrect, or you need to implement the `CLIPEncoderLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling CLIPAttention.call().\n\n\u001b[1mpred must not be a Python bool\u001b[0m\n\nArguments received by CLIPAttention.call():\n  • inputs=tf.Tensor(shape=(None, 77, 768), dtype=float32)\n  • attention_mask=None\u001b[0m\n\nArguments received by CLIPEncoderLayer.call():\n  • args=('<KerasTensor shape=(None, 77, 768), dtype=float32, sparse=False, name=keras_tensor_3>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "images = model.text_to_image(\"photograph of an astronaut riding a horse\", batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.text_to_image(\n",
    "    \"cute magical flying dog, fantasy art, \"\n",
    "    \"golden color, high quality, highly detailed, elegant, sharp focus, \"\n",
    "    \"concept art, character concepts, digital painting, mystery, adventure\",\n",
    "    batch_size=3,\n",
    ")\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
